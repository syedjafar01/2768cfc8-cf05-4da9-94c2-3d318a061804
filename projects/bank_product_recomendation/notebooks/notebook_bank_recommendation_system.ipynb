{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nwc-6Be5IgNL"
   },
   "outputs": [],
   "source": [
    "import projectFunctions as pF\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "random_seed = 206"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K1ACJQQrIgNS"
   },
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "df_train = pF.loadData(\"project-space/bank-product-recommendation/data/train.txt\")\n",
    "df_train.head()\n",
    "\n",
    "df_test = pF.loadData(\"project-space/bank-product-recommendation/data/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MUhRFho754l_",
    "outputId": "5adaf294-ad90-45d3-c4c4-f7e9f9d2b268"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3350601, 8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "irnxdVNgydTB",
    "outputId": "61ccf205-1e70-4fb9-cbe3-45db5f170834"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "676370 users\n",
      "94 products\n"
     ]
    }
   ],
   "source": [
    "n_users = df_train.ID_Customer.unique().shape[0]\n",
    "n_items = df_train['Cod_Prod'].unique().shape[0]\n",
    "print(str(n_users) + ' users') \n",
    "print(str(n_items) + ' products') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-jRmdYqnIgNV"
   },
   "source": [
    "<a id=\"transf\"></a>\n",
    "## **Preprocessing data**\n",
    "\n",
    "In the next cell we carry out the transformation of the data with the two functions:\n",
    "\n",
    "- ***tratamientoFecha***: (Datetreatment) is responsible for adding a new attribute corresponding to the number of days that have passed since 01-01-1950 for each record. \n",
    "\n",
    "\n",
    "- ***mapProduct:*** which is responsible for mapping the products to integer values ordered by their frequency of appearance in the training file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wp20HKnJIgNW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/projectFunctions.py:26: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  primerMes = pd.datetime(1950, 1, 1, 0, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "# Train transformation\n",
    "df_train = pF.tratamientoFecha(df_train)\n",
    "df_train = pF.mapProduct(df_train)\n",
    "\n",
    "# Test transformacion\n",
    "df_test = pF.tratamientoFecha(df_test)\n",
    "df_test = pF.mapProdByDict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7COWTdqIgNZ"
   },
   "source": [
    "We use all the data for the creation of the different maps used for the generation of new features:\n",
    "\n",
    "- ***mapAparicionProduct:*** generates a map of dates when the different products of the training set appear, assigning an ascending whole value.\n",
    "\n",
    "- ***mapDiasInicio:*** generates a map to which you assign each training and test user the day on which you made the first purchase. This will be used to estimate the number of days since you registered in the database until you purchased the last product.\n",
    "\n",
    "- ***mapYearPIB:*** loads a map of annual GDP per capita Spanish that must be in the same folder as this Notebook, whose name must be PIB.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BDQelYZTIgNZ"
   },
   "outputs": [],
   "source": [
    "__ = pF.mapAparicionProd(df_train)\n",
    "__ = pF.mapDiasInicio(df_train)\n",
    "__ = pF.mapDiasInicio(df_test)\n",
    "__ = pF.mapYearPIB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TXzSpy7nIgNc"
   },
   "source": [
    "### **Treatment of the time series**\n",
    "\n",
    "In the next cell we will include as attributes the previous products purchased by the users, as well as the number of days elapsed since 01-01-1950 until the acquisition of each of the previous products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uMX4Nvp7IgNd"
   },
   "outputs": [],
   "source": [
    "# Number of previous products to considerate\n",
    "nant = 8\n",
    "incluir_num_dias = True\n",
    "\n",
    "# Train\n",
    "df_train = pF.addProdAnt(df_train, nant, incluir_num_dias)\n",
    "\n",
    "# Test\n",
    "df_test = pF.addProdAnt(df_test, nant, incluir_num_dias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V7-tjvo-m3U3"
   },
   "source": [
    "#### **Dataframe creation**\n",
    "In the next cell we create three dataframes of the time series:\n",
    "- *dfTr2model*: this training dataframe is what we will use to generate the prediction model.\n",
    "- *dfTs2eval*: this test dataframe will be used to evaluate the results of the prediction. In this way, it will help us estimate how we will do it in prediction.\n",
    "- *dfTs2predict*: this test dataframe is what we will use to carry out the prediction. The chosen data corresponds to the last entry for each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yELYpJNpnIa_"
   },
   "outputs": [],
   "source": [
    "# Training subset\n",
    "dfTr2model = pF.ultimoElementoSerie(df_train)\n",
    "\n",
    "# Evaluation subset\n",
    "dfTs2eval = pF.ultimoElementoSerie(df_test)\n",
    "\n",
    "# Prediction subset\n",
    "dfTs2predict = pF.createTest(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zfpOW4V_nU6Y"
   },
   "source": [
    "We add the number of products purchased as a feature. For the prediction dataframe we indicate that you take all the products purchased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tmsd6zoznYBk"
   },
   "outputs": [],
   "source": [
    "dfTr2model[\"Num_Prod_Ant\"] = pF.numProductosComprados(df_train)\n",
    "dfTs2eval[\"Num_Prod_Ant\"] = pF.numProductosComprados(df_train)\n",
    "dfTs2predict[\"Num_Prod_Ant\"] = pF.numProductosComprados(df_train, test = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "guORvO4fnsMX"
   },
   "source": [
    "### Dataset reduction\n",
    "\n",
    "**Sub-sampling**\n",
    "\n",
    "Due to the large amount of data provided in the training file, we performed a random sub-sampling of the * train * dataframe in order to carry out the training of the models in a reasonable time.\n",
    "This sub-sampling is performed in a stratified manner, as explained in the report.\n",
    "In our case we are left with 20% of the train data, which is more than 120,000 records.\n",
    "\n",
    "Once the training model to be implemented in the validation phase has been chosen, in order to estimate the goodness of the classifier on the test set, it is decided to evaluate the success rate on this set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "1R7QaADEnotv",
    "outputId": "3185b34d-1964-45ee-e73b-ff2d513bf4b3"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c563ceef30d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Training subset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m.2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdfTr2model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCod_Prod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Test subset, for evaluation on this data set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pF' is not defined"
     ]
    }
   ],
   "source": [
    "# Training subset\n",
    "size = .2\n",
    "dfTr2model = pF.subset(df_train, df_train.Cod_Prod, size = size)\n",
    "\n",
    "# Test subset, for evaluation on this data set\n",
    "size = .2\n",
    "dfTs2eval = pF.subset(df_test, df_test.Cod_Prod, size = size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ehkFSBzwn9T0"
   },
   "source": [
    "**Elimination of minority classes**\n",
    "\n",
    "In order to eliminate the minority classes of the training set we *prune* it.\n",
    "\n",
    "For the test case it would not be necessary because you do not need to model any class with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "Md03GvfBn-oc",
    "outputId": "43b44f19-44c5-41a2-fa64-286f9e633945"
   },
   "outputs": [],
   "source": [
    "clases = 60\n",
    "resto = False\n",
    "\n",
    "dfTr2model = pF.classPrune2(dfTr2model, dfTr2model.Cod_Prod, nc = clases, resto = resto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N8IwqPPkIgN1"
   },
   "source": [
    "\n",
    "\n",
    "<a id=\"feat\"></a>\n",
    "## Treatment of features\n",
    "\n",
    "**New features**\n",
    "\n",
    "In the next cell we add the characteristics to the data sets. Some of these functions use the maps loaded at the beginning of the Notebook. This process may take a while to run, but not more than 5 minutes (depends on the computer and the amount of data).\n",
    "\n",
    "- **restaFechas**: is responsible for subtracting the days between the previous products that each customer has purchased and creating a corresponding column.\n",
    "\n",
    "\n",
    "- **acontecimiento**: creates a column with the event of Cajamar according to the time in which the user buys the previous product. The event is marked by the merger of the rural banks.\n",
    "\n",
    "\n",
    "- **addPIBAnt**: creates a column with the GDP per capita of the years in which the user buys the previous product.\n",
    "\n",
    "\n",
    "- **addDiasInicioAnt**: create a column with the days that have passed since the user started buying, taking into account the date of purchase of the previous product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "Mr0lYav5IgN3",
    "outputId": "9f813717-be8d-4f49-f56e-8c5cb7475b97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creation of features for training set completed.\n",
      "Creation of features for evaluation set completed.\n",
      "Creation of features for prediction set completed.\n"
     ]
    }
   ],
   "source": [
    "dfTr2model = pF.restaFechas(dfTr2model)\n",
    "dfTr2model = pF.acontecimiento(dfTr2model)\n",
    "dfTr2model = pF.addPIBAnt(dfTr2model)\n",
    "dfTr2model = pF.addDiasInicioAnt(dfTr2model)\n",
    "print(\"Creation of features for training set completed.\")\n",
    "\n",
    "dfTs2eval = pF.restaFechas(dfTs2eval)\n",
    "dfTs2eval = pF.acontecimiento(dfTs2eval)\n",
    "dfTs2eval = pF.addPIBAnt(dfTs2eval)\n",
    "dfTs2eval = pF.addDiasInicioAnt(dfTs2eval)\n",
    "print(\"Creation of features for evaluation set completed.\")\n",
    "\n",
    "dfTs2predict = pF.restaFechas(dfTs2predict)\n",
    "dfTs2predict = pF.acontecimiento(dfTs2predict)\n",
    "dfTs2predict = pF.addPIBAnt(dfTs2predict)\n",
    "dfTs2predict = pF.addDiasInicioAnt(dfTs2predict)\n",
    "print(\"Creation of features for prediction set completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZWg6fFVPIgN5"
   },
   "source": [
    "**Expansion of variables**\n",
    "\n",
    "In the next cell we carry out the expansion of categorical variables, in which as many columns as possible values can be created the variable passed by argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "xN14BlGUIgN6",
    "outputId": "1514ec4c-9b49-4628-d02c-590d25382331"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanding Socio_Demo_05 to dimension 4\n",
      "Expanding Socio_Demo_05 to dimension 4\n",
      "Expanding Socio_Demo_05 to dimension 4\n"
     ]
    }
   ],
   "source": [
    "variable = \"Socio_Demo_05\"\n",
    "\n",
    "dfTr2model = pF.expandirVariable(dfTr2model, variable)\n",
    "dfTs2eval = pF.expandirVariable(dfTs2eval, variable)\n",
    "dfTs2predict = pF.expandirVariable(dfTs2predict, variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BiwsI_eLv7vN"
   },
   "source": [
    "#### **Feature selection**\n",
    "\n",
    "In the next cell we eliminate attributes that have served us to calculate other variables but that we do not really have at the time of making the prediction.\n",
    "\n",
    "The attributes to eliminate in this part are:\n",
    "\n",
    "- **Cod_Fecha**: this attribute is eliminated, since in the real scenario we can not know when the user is going to buy the new product.\n",
    "\n",
    "\n",
    "- **Cod_Fecha_Ant**: it is a column in date format when I bought the previous product, we eliminated it because we have already made the relevant transformations, and it is not useful in date format.\n",
    "\n",
    "\n",
    "- **Num_Dias**: this column is not available because we do not have the current purchase date to know how many days have passed since 1950."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MKL7oZUKrGX4"
   },
   "outputs": [],
   "source": [
    "columns2drop = [\"Cod_Fecha\", \"Cod_Fecha_Ant\", \"Num_Dias\"]\n",
    "\n",
    "dfTr2model = dfTr2model.drop(columns2drop, axis=1)\n",
    "dfTs2eval = dfTs2eval.drop(columns2drop, axis=1)\n",
    "dfTs2predict = dfTs2predict.drop(columns2drop,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_p0VRGZcv9FW"
   },
   "source": [
    "## **Machine Learning model**\n",
    "In the next cell we extract the columns with which we will carry out the training of our classifier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "8kf_6A4TrT43",
    "outputId": "1863d3c3-1c87-4da6-9303-d7d31b020757"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Socio_Demo_01', 'Socio_Demo_02', 'Socio_Demo_03', 'Socio_Demo_04', 'Cod_Prod_Ant1', 'Num_Dias_Ant1', 'Cod_Prod_Ant2', 'Num_Dias_Ant2', 'Cod_Prod_Ant3', 'Num_Dias_Ant3', 'Cod_Prod_Ant4', 'Num_Dias_Ant4', 'Cod_Prod_Ant5', 'Num_Dias_Ant5', 'Cod_Prod_Ant6', 'Num_Dias_Ant6', 'Cod_Prod_Ant7', 'Num_Dias_Ant7', 'Cod_Prod_Ant8', 'Num_Dias_Ant8', 'Diferencia_Fechas_1', 'Diferencia_Fechas_2', 'Diferencia_Fechas_3', 'Diferencia_Fechas_4', 'Diferencia_Fechas_5', 'Diferencia_Fechas_6', 'Diferencia_Fechas_7', 'AcontecimientoAnt', 'PIB_Ant', 'DiasDesde_Inicio', 'Socio_Demo_05_1', 'Socio_Demo_05_2', 'Socio_Demo_05_3', 'Socio_Demo_05_4']\n"
     ]
    }
   ],
   "source": [
    "# We choose the training data by eliminating the product code and the user ID.\n",
    "columns = list(dfTr2model.columns[2:len(dfTr2model.columns)])\n",
    "\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lz_PkQe8rTS4"
   },
   "source": [
    "<a id=\"train\"> </a>\n",
    "## **Training**\n",
    "\n",
    "In the next cell we carry out the entry of our classifier *XGBoost* with the optimal parameters obtained from the validation phase. \n",
    "\n",
    "***Note: We should do a tuning of parameters to find the best values that improve the model.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5eRUWErqrn_l",
    "outputId": "10432235-054e-4b14-c05b-71d914b175dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processors:  40\n"
     ]
    }
   ],
   "source": [
    "# Uncoment and comment from import ... If we have a trained model file\n",
    "# we can load it with:\n",
    "#import pickle\n",
    "#estimator = pickle.load(open(\"trained_model.pickle.dat\", \"rb\"))\n",
    "\n",
    "import multiprocessing\n",
    "nproc = multiprocessing.cpu_count() # For a fastest train, we use all cpu processors\n",
    "print('Number of processors: ',nproc)\n",
    "estimator=XGBClassifier(learning_rate = 0.1,\n",
    "                       nthread = nproc,\n",
    "                       base_score = 0.2,\n",
    "                       n_estimators = 200,\n",
    "                       seed = random_seed,\n",
    "                       max_depth = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "osOHQ1kWxZxh",
    "outputId": "619a1dfa-adb4-42e1-e2e2-bc5f5cb986b9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.2, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=8, min_child_weight=1, missing=None, n_estimators=200,\n",
       "       n_jobs=1, nthread=40, objective='multi:softprob', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=206,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.fit(dfTr2model[columns], dfTr2model.Cod_Prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wm89dHJWJnLZ"
   },
   "outputs": [],
   "source": [
    "# Uncomment if we want save model to file\n",
    "#import pickle\n",
    "#pickle.dump(estimator, open(\"trained_model.pickle.dat\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mWAqtMA9sE2F"
   },
   "source": [
    "**Evaluation of the test data**\n",
    "\n",
    "In order to observe the results of the test predictions, the trained classifier is evaluated on the subset of test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TkGWwuuRsFd-"
   },
   "outputs": [],
   "source": [
    "tsScore = estimator.score(dfTs2eval[columns], dfTs2eval.Cod_Prod)\n",
    "\n",
    "print(\"Score obtained in test: \"+ str(tsScore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Al-N1cIesVsP"
   },
   "source": [
    "<a id=\"predict\"> </a>\n",
    "## **Prediction**\n",
    "\n",
    "We make the prediction of the future products to be hired by the customers of the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qmF0Rz_bsXDF",
    "outputId": "a63db964-5a41-4280-de7a-e65981f2c8a8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enigma-admin/.local/lib/python3.5/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "Cod_Prod_predicted = estimator.predict(dfTs2predict[columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8B64JNpnsbM2"
   },
   "source": [
    "**Creation of the results dataframe**\n",
    "\n",
    "In the next cell, the creation of a dataframe with the customer's ID and the product code to be purchased is carried out.\n",
    "\n",
    "Subsequently, an inverse mapping of these product codes is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZmH3AgUYsalS",
    "outputId": "43095cd3-6dd1-4095-c40f-276fbebd49cf"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Cod_Prod_predicted' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-d012956a40cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# We create a new column with the prediction.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdfresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdfTs2predict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ID_Customer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mCod_Prod_predicted\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ID_Customer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cod_Prod\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# The mapping of the products is carried out in reverse.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdfresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetDfMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Cod_Prod_predicted' is not defined"
     ]
    }
   ],
   "source": [
    "# We create a new column with the prediction.\n",
    "dfresults = pd.DataFrame([dfTs2predict[\"ID_Customer\"],Cod_Prod_predicted],[\"ID_Customer\", \"Cod_Prod\"]).transpose()\n",
    "\n",
    "# The mapping of the products is carried out in reverse.\n",
    "dfresults = pF.getDfMap(dfresults)\n",
    "\n",
    "dfresults.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ztFpQoTCsuxf"
   },
   "source": [
    "**Export of results**\n",
    "\n",
    "Finally, we export the dataframe with the results to the output format: ID_Customer, Cod_Prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bqygLnHnstGn"
   },
   "outputs": [],
   "source": [
    "fileName = \"output_prediction.csv\"\n",
    "dfresults.to_csv(fileName, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "notebook_bank_recommendation_system.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
